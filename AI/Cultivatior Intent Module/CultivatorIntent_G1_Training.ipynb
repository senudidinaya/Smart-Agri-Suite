{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14xW3FhCdbQEWuGJ2H00crVuNhCsV3MCu",
      "authorship_tag": "ABX9TyP+07xXilYewUFVrEeRERgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/senudidinaya/Smart-Agri-Suite/blob/main/AI/Cultivator%20Intention%20Module/CultivatorIntent_G1_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8rfFdNwmwxa",
        "outputId": "50cb6cee-af2c-4996-dbb1-13ce6927f1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ],
      "source": [
        "KAGGLE_USERNAME = \"senudirupasinghe\"\n",
        "KAGGLE_KEY = \"7780e1bc02634783fb08137fa45db94e\"\n",
        "\n",
        "import os, json, pathlib\n",
        "pathlib.Path(\"/root/.kaggle\").mkdir(parents=True, exist_ok=True)\n",
        "with open(\"/root/.kaggle/kaggle.json\",\"w\") as f:\n",
        "    json.dump({\"username\":KAGGLE_USERNAME,\"key\":KAGGLE_KEY}, f)\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
        "\n",
        "!pip -q install kaggle datasets soundfile librosa\n",
        "!kaggle --version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --no-cache-dir \"huggingface_hub==0.25.2\" \"datasets==2.20.0\" soundfile librosa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjcrmCc43kYt",
        "outputId": "b433d18e-b820-4fd1-9f70-0e7f80b09fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/436.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.6/436.6 kB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/547.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/316.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m179.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.49.1 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.25.2 which is incompatible.\n",
            "diffusers 0.35.2 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.25.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\n",
            "transformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 0.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = getpass(\"Paste your Hugging Face token (starts with hf_): \")\n",
        "login(token=hf_token)   # stores the token for this session\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Z8yZ4Z4-Dv",
        "outputId": "dc077922-2ee2-4a3a-ed7e-a018641bafb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your Hugging Face token (starts with hf_): ··········\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download anchors\n",
        "# Sri-Lankan English (accent):\n",
        "!mkdir -p /content/data/sri_lanka_en\n",
        "!kaggle datasets download -d chamodsr/sri-lankan-accent-voice-data-setenglish -p /content/data/sri_lanka_en --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP66M6P75cAE",
        "outputId": "01c01e87-f6e5-4582-999f-3ce3a4747277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/chamodsr/sri-lankan-accent-voice-data-setenglish\n",
            "License(s): unknown\n",
            "Downloading sri-lankan-accent-voice-data-setenglish.zip to /content/data/sri_lanka_en\n",
            "  0% 0.00/15.4M [00:00<?, ?B/s]\n",
            "100% 15.4M/15.4M [00:00<00:00, 1.19GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Voice (Sinhala + English) via Kaggle big pack (then filter), or use HF datasets (faster):\n",
        "# Hugging Face route\n",
        "!kaggle datasets list -s \"MELD\" | head -n 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVVa-AMH6Yvm",
        "outputId": "e46c6c20-8faf-498b-cc91-f9c8b629ce98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                        title                                         size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------  -------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "zaber666/meld-dataset                                      Multimodal EmotionLines Dataset(MELD)  11804662112  2021-02-26 14:05:51.470000           5990         38  0.6875           \n",
            "argish/meld-preprocessed                                   MELD Preprocessed                       3527202381  2025-03-01 06:45:24.590000            502          2  0.625            \n",
            "bhandariprakanda/meld-emotion-recognition                  MELD: Emotion Recognition Dataset      10885202576  2025-02-12 10:13:09.047000            317          1  0.64705884       \n",
            "mathurinache/pretrained-models-emotion-recognition         Pretrained Models Emotion Recognition     87483853  2020-09-04 06:13:28.177000            662         16  0.6875           \n",
            "sagnikdebnath007/meld-embeddings                           Meld embeddings                         3826286328  2024-10-31 19:18:24.690000             16          0  0.4117647        \n",
            "alimistro123/meld-dataset-excel-only                       MELD_DATASET_EXCEL_ONLY                     515866  2023-11-12 12:56:31.397000             24          3  0.1764706        \n",
            "gunachalla/meld-data                                       meld-data                                313588525  2025-11-01 15:36:01.050000              3          0  0.4117647        \n",
            "bhandariprakanda/meld-audio                                MELD Audio                              6029462858  2025-01-23 19:10:51.490000             14          0  0.375            \n",
            "prakandabhandari/video-frames-v1                           MELD Video Frames                       1346555939  2025-05-06 17:35:28.920000             13          0  0.3125           \n",
            "vincentosere/meldata                                       Meldata                                     461423  2025-04-16 13:02:03.620000              5          1  0.1764706        \n",
            "unmol03/meld-mapping-with-mbti                             MELD_MAPPING_WITH_MBTI                  2269382592  2024-01-22 09:27:25.273000             17          1  0.23529412       \n",
            "vanshikavmittal/meld-output-faces                          MELD-output-faces                        143227105  2025-05-15 12:57:23.660000             12          0  0.25             \n",
            "seniruepasinghe/meld-emotion-detection-preprocessed        MELD-emotion-detection-preprocessed     4536497614  2025-11-01 12:13:09.140000              4          0  0.47058824       \n",
            "chafikboulealam/meld-files                                 MELD_files                                   10521  2021-09-20 21:39:07.953000            326          0  0.0625           \n",
            "daiphuocvo/meld-multimodal-speech-text-sentiment-analysis  MELD_dataset                           10880813838  2024-05-06 23:26:46.997000            190          0  0.29411766       \n",
            "rumaiya/iemocap                                            IEMOCAP & MELD                         29650485528  2024-06-20 22:29:05.067000             49          0  0.125            \n",
            "ramprakashrajendran/meld-files                             meld-files                                    2159  2024-11-16 19:12:02.217000              2          0  0.125            \n",
            "vesperxyc/meld-master                                      MELD-master                                2987661  2025-10-09 13:03:23.587000              0          0  0.125            \n",
            "ngocson2002/meld-datasetv2                                 MELD-datasetv2                          1526730800  2024-10-16 06:59:45.853000              4          0  0.11764706       \n",
            "xylarwardhan/pickles-for-audio-of-meld                     Processed MELD files                     104590534  2022-12-08 18:58:01.490000             25          0  0.23529412       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install + download MELD with kagglehub\n",
        "!pip -q install kagglehub\n",
        "import kagglehub, os, glob"
      ],
      "metadata": {
        "id": "NJOnlMChPjeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"zaber666/meld-dataset\")  # returns local folder\n",
        "print(\"Local path:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGomalUjSoSQ",
        "outputId": "6d78f384-cc00-4b72-b23c-96c35f144ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/zaber666/meld-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11.0G/11.0G [08:17<00:00, 23.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local path: /root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See what actually came down\n",
        "csvs = glob.glob(os.path.join(path,\"**\",\"*sent_emo*.csv\"), recursive=True)\n",
        "mp4s = glob.glob(os.path.join(path,\"**\",\"*.mp4\"), recursive=True)\n",
        "zips = glob.glob(os.path.join(path,\"**\",\"*.zip\"), recursive=True)\n",
        "print(\"CSV files:\", len(csvs), \" | MP4 files:\", len(mp4s), \" | ZIPs:\", len(zips))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb3iJMCjCjTB",
        "outputId": "4866f9ef-e21b-4dba-bcdf-7dd47d1d43a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV files: 3  | MP4 files: 13848  | ZIPs: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MELD_ROOT = path  # run the discovery + ensure_wav + build_manifest cells"
      ],
      "metadata": {
        "id": "PWZlZDMyC6JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "train_csv = glob.glob(os.path.join(MELD_ROOT,\"**\",\"train_sent_emo.csv\"), recursive=True)\n",
        "dev_csv   = glob.glob(os.path.join(MELD_ROOT,\"**\",\"dev_sent_emo.csv\"),   recursive=True)\n",
        "test_csv  = glob.glob(os.path.join(MELD_ROOT,\"**\",\"test_sent_emo.csv\"),  recursive=True)\n",
        "mp4s      = glob.glob(os.path.join(MELD_ROOT,\"**\",\"*.mp4\"), recursive=True)\n",
        "print(\"CSV:\", bool(train_csv), bool(dev_csv), bool(test_csv), \"| MP4:\", len(mp4s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahELZsctE0y2",
        "outputId": "31d9f3ca-b965-4de2-e947-cfbf675237c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV: True True True | MP4: 13848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt -y install -qq ffmpeg\n",
        "import os, glob, subprocess, pandas as pd, pathlib\n",
        "\n",
        "MELD_ROOT = MELD_ROOT  # keep whatever you set earlier\n",
        "AUDIO_OUT = \"/content/audio/meld\"\n",
        "os.makedirs(AUDIO_OUT, exist_ok=True)\n",
        "\n",
        "# Build a fast lookup once\n",
        "mp4_index = {}\n",
        "for p in glob.glob(os.path.join(MELD_ROOT, \"**\", \"*.mp4\"), recursive=True):\n",
        "    mp4_index[os.path.basename(p).lower()] = p\n",
        "print(\"MP4 files indexed:\", len(mp4_index))\n",
        "\n",
        "def mp4_name(dialogue_id:int, utterance_id:int):\n",
        "    return f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
        "\n",
        "def ensure_wav(dialogue_id:int, utterance_id:int):\n",
        "    src = mp4_index.get(mp4_name(dialogue_id, utterance_id).lower())\n",
        "    if not src:\n",
        "        return None\n",
        "    dst = os.path.join(AUDIO_OUT, f\"{dialogue_id}_{utterance_id}.wav\")\n",
        "    if not os.path.exists(dst):\n",
        "        subprocess.run(\n",
        "            [\"ffmpeg\",\"-y\",\"-i\",src,\"-vn\",\"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\",dst],\n",
        "            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
        "        )\n",
        "    return dst if os.path.exists(dst) else None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Rlj31YF5jp",
        "outputId": "3121d672-de74-4e3c-84ba-0b5579c5e357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "MP4 files indexed: 11264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity check (1 clip)\n",
        "import pandas as pd, glob, os\n",
        "\n",
        "# locate CSVs\n",
        "def find_csv(name, root):\n",
        "    hits = glob.glob(os.path.join(root,\"**\",name), recursive=True)\n",
        "    assert hits, f\"{name} not found under {root}\"\n",
        "    return hits[0]\n",
        "\n",
        "train_csv = find_csv(\"train_sent_emo.csv\", MELD_ROOT)\n",
        "df = pd.read_csv(train_csv)\n",
        "\n",
        "# try the first row\n",
        "d, u = int(df.iloc[0][\"Dialogue_ID\"]), int(df.iloc[0][\"Utterance_ID\"])\n",
        "wav_path = ensure_wav(d, u)\n",
        "print(\"WAV created:\", wav_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8gxzKFmGoSV",
        "outputId": "26f60a4d-1899-4e61-80a8-6bff8094b338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WAV created: /content/audio/meld/0_0.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build manifests (with progress bars)\n",
        "!pip -q install tqdm\n",
        "from tqdm import tqdm\n",
        "import pandas as pd, os, glob\n",
        "\n",
        "emap = {\n",
        "    \"engaged\":    {\"neutral\",\"joy\",\"happy\",\"excited\"},\n",
        "    \"hesitating\": {\"sadness\",\"fear\",\"surprise\"},\n",
        "    \"rejecting\":  {\"anger\",\"disgust\",\"contempt\"},\n",
        "}\n",
        "intent_id = {\"engaged\":0,\"hesitating\":1,\"rejecting\":2}\n",
        "def emotion_to_intent(e):\n",
        "    e = str(e).strip().lower()\n",
        "    for k,v in emap.items():\n",
        "        if e in v: return intent_id[k]\n",
        "    return None\n",
        "\n",
        "def build_manifest(csv_path, tag):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    D,U,E = cols[\"dialogue_id\"], cols[\"utterance_id\"], cols[\"emotion\"]\n",
        "    rows = []\n",
        "    for d,u,e in tqdm(zip(df[D], df[U], df[E]), total=len(df), desc=f\"Extract {tag}\"):\n",
        "        p = ensure_wav(int(d), int(u))\n",
        "        y = emotion_to_intent(e)\n",
        "        if p and y is not None:\n",
        "            rows.append({\"path\": p, \"label\": y, \"lang\": \"en\", \"domain\": \"general\"})\n",
        "    man = pd.DataFrame(rows)\n",
        "    print(f\"{tag}: kept {len(man)} / {len(df)}\")\n",
        "    return man\n",
        "\n",
        "dev_csv  = find_csv(\"dev_sent_emo.csv\", MELD_ROOT)\n",
        "test_csv = find_csv(\"test_sent_emo.csv\", MELD_ROOT)\n",
        "\n",
        "man_train = build_manifest(train_csv, \"train\")\n",
        "man_dev   = build_manifest(dev_csv,   \"dev\")\n",
        "man_test  = build_manifest(test_csv,  \"test\")\n",
        "\n",
        "man_train.to_csv(\"/content/manifest_meld_train.csv\", index=False)\n",
        "man_dev.to_csv(\"/content/manifest_meld_dev.csv\", index=False)\n",
        "man_test.to_csv(\"/content/manifest_meld_test.csv\", index=False)\n",
        "\n",
        "len(man_train), len(man_dev), len(man_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK8jUBJtITFs",
        "outputId": "ea25651d-01c5-4d3a-9829-74be7cd076d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extract train: 100%|██████████| 9989/9989 [15:14<00:00, 10.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: kept 9989 / 9989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extract dev: 100%|██████████| 1109/1109 [00:55<00:00, 19.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev: kept 1109 / 1109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extract test: 100%|██████████| 2610/2610 [01:30<00:00, 28.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test: kept 2610 / 2610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9989, 1109, 2610)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependancies for Phase-A\n",
        "!pip -q install opensmile torch torchaudio transformers tqdm scikit-learn soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyO5t5wMNsRH",
        "outputId": "53b66ba5-00c3-462f-c893-74581d35e25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/566.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.8/137.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wav2Vec2Processor is trying to load a tokenizer (needs a vocab.json), but facebook/wav2vec2-large-xlsr-53 doesn’t ship one—it’s a feature-extractor-only model. Use the feature extractor instead of the processor."
      ],
      "metadata": {
        "id": "AwASoAyyPoVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install stable versions\n",
        "!pip -q install --upgrade --no-cache-dir \"transformers==4.39.3\" \"huggingface_hub>=0.23\" accelerate\n",
        "\n",
        "# 3) Use the feature extractor API (not AutoFeatureExtractor)\n",
        "import torch, numpy as np\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fe   = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "xlsr = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\").to(device).eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def xlsr_embed(y, sr=16000):\n",
        "    inputs = fe(y, sampling_rate=sr, return_tensors=\"pt\")\n",
        "    hs = xlsr(inputs.input_values.to(device)).last_hidden_state\n",
        "    return hs.mean(dim=1).squeeze(0).cpu().numpy().astype(np.float32)"
      ],
      "metadata": {
        "id": "oDA1qZXYPc1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on \"Loading Model completion\"\n",
        "# Assuming your xlsr_embed func is defined\n",
        "import torchaudio\n",
        "if 'man_train' in globals():\n",
        "    sample_audio, sr = torchaudio.load(man_train['path'].iloc[0])\n",
        "    embed = xlsr_embed(sample_audio.numpy().squeeze(), sr)\n",
        "    print(\"Success! Embed shape:\", embed.shape)  # Expect (1024,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaqEboQvVXry",
        "outputId": "631009c9-0688-4ca5-cb25-aa29e197bbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Embed shape: (1024,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple feature builder: XLSR only\n",
        "import numpy as np\n",
        "\n",
        "def featurize(path):\n",
        "    y = load_wav(path, 16000).astype(np.float32)\n",
        "    return xlsr_embed(y, 16000)  # -> (1024,)"
      ],
      "metadata": {
        "id": "eB2YtrQLOLph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers you need in the current runtime\n",
        "import soundfile as sf, torch, torchaudio\n",
        "\n",
        "def load_wav(path, sr=16000):\n",
        "    y, s = sf.read(path, always_2d=False)\n",
        "    # mono\n",
        "    if isinstance(y, np.ndarray) and y.ndim > 1:\n",
        "        y = y.mean(-1)\n",
        "    # resample if needed\n",
        "    if s != sr:\n",
        "        y = torchaudio.functional.resample(\n",
        "            torch.tensor(y, dtype=torch.float32).unsqueeze(0), s, sr\n",
        "        ).squeeze(0).numpy()\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "# XLSR-only feature (1024-d)\n",
        "def featurize(path):\n",
        "    y = load_wav(path, 16000)\n",
        "    return xlsr_embed(y, 16000)  # uses the XLSR embed function you already defined"
      ],
      "metadata": {
        "id": "nRjNQXX1aGeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick check\n",
        "import pandas as pd\n",
        "man_train = pd.read_csv(\"/content/manifest_meld_train.csv\")\n",
        "print(\"Feature shape:\", featurize(man_train['path'].iloc[0]).shape)  # should be (1024,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttGyl48WbNgt",
        "outputId": "1c584069-9476-46af-c59b-a137a4c66fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: (1024,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build NPZs\n",
        "from tqdm import tqdm\n",
        "import pandas as pd, numpy as np, os\n",
        "\n",
        "FEAT_DIR = \"/content/features\"; os.makedirs(FEAT_DIR, exist_ok=True)\n",
        "\n",
        "def build_npz(manifest_csv, out_npz):\n",
        "    man = pd.read_csv(manifest_csv)\n",
        "    X, y = [], []\n",
        "    for p, lab in tqdm(zip(man[\"path\"], man[\"label\"]), total=len(man), desc=f\"Featurize {os.path.basename(manifest_csv)}\"):\n",
        "        x = featurize(p); X.append(x); y.append(int(lab))\n",
        "    X = np.stack(X).astype(np.float32)\n",
        "    y = np.array(y, dtype=np.int64)\n",
        "    np.savez(out_npz, X=X, y=y)\n",
        "    print(out_npz, X.shape, y.shape)\n",
        "\n",
        "build_npz(\"/content/manifest_meld_train.csv\", \"/content/features/meld_train.npz\")\n",
        "build_npz(\"/content/manifest_meld_dev.csv\",   \"/content/features/meld_dev.npz\")\n",
        "build_npz(\"/content/manifest_meld_test.csv\",  \"/content/features/meld_test.npz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umzJVorabRb2",
        "outputId": "dce839f9-0f66-4673-aa7d-8cf0dc95af0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Featurize manifest_meld_train.csv: 100%|██████████| 9989/9989 [09:13<00:00, 18.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/features/meld_train.npz (9989, 1024) (9989,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Featurize manifest_meld_dev.csv: 100%|██████████| 1109/1109 [01:00<00:00, 18.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/features/meld_dev.npz (1109, 1024) (1109,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Featurize manifest_meld_test.csv: 100%|██████████| 2610/2610 [02:39<00:00, 16.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/features/meld_test.npz (2610, 1024) (2610,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the head (1024 → 256 → 64 → 3)\n",
        "import numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_npz(p):\n",
        "    d=np.load(p); return d[\"X\"].astype(np.float32), d[\"y\"].astype(np.int64)\n",
        "\n",
        "Xtr,ytr = load_npz(\"/content/features/meld_train.npz\")\n",
        "Xdv,ydv = load_npz(\"/content/features/meld_dev.npz\")\n",
        "\n",
        "tr = torch.utils.data.TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr))\n",
        "dv = torch.utils.data.TensorDataset(torch.from_numpy(Xdv), torch.from_numpy(ydv))\n",
        "train_loader = torch.utils.data.DataLoader(tr, batch_size=64, shuffle=True)\n",
        "dev_loader   = torch.utils.data.DataLoader(dv, batch_size=128, shuffle=False)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, d_in=1024, d_h=256, d_h2=64, n_out=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_in,d_h), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(d_h,d_h2), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(d_h2,n_out)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "model = Head().to(device)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "opt   = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
        "\n",
        "best_f1, wait, patience = 0.0, 0, 5\n",
        "for ep in range(40):\n",
        "    model.train()\n",
        "    for xb,yb in train_loader:\n",
        "        xb,yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad(); loss = crit(model(xb), yb); loss.backward(); opt.step()\n",
        "    # dev\n",
        "    model.eval(); preds=[]; gold=[]\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in dev_loader:\n",
        "            pb = model(xb.to(device)).softmax(1).argmax(1).cpu().numpy()\n",
        "            preds.extend(pb); gold.extend(yb.numpy())\n",
        "    f1 = f1_score(gold, preds, average=\"macro\")\n",
        "    print(f\"Epoch {ep:02d}  dev Macro-F1 = {f1:.3f}\")\n",
        "    if f1>best_f1: best_f1, wait = f1, 0; torch.save(model.state_dict(), \"/content/features/head_best.pt\")\n",
        "    else: wait += 1\n",
        "    if wait>=patience: print(\"Early stop.\"); break\n",
        "\n",
        "print(\"Best dev Macro-F1:\", round(best_f1,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaQLOy6Ufq_a",
        "outputId": "1fb9f17a-1099-4c98-c066-3ff045d05dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00  dev Macro-F1 = 0.242\n",
            "Epoch 01  dev Macro-F1 = 0.242\n",
            "Epoch 02  dev Macro-F1 = 0.242\n",
            "Epoch 03  dev Macro-F1 = 0.242\n",
            "Epoch 04  dev Macro-F1 = 0.242\n",
            "Epoch 05  dev Macro-F1 = 0.242\n",
            "Early stop.\n",
            "Best dev Macro-F1: 0.242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Xte,yte = load_npz(\"/content/features/meld_test.npz\")\n",
        "te_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(torch.from_numpy(Xte), torch.from_numpy(yte)),\n",
        "    batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/features/head_best.pt\", map_location=device))\n",
        "model.eval(); preds=[]; gold=[]; probs=[]\n",
        "with torch.no_grad():\n",
        "    for xb,yb in te_loader:\n",
        "        pr = model(xb.to(device)).softmax(1).cpu().numpy()\n",
        "        probs.append(pr); preds.extend(pr.argmax(1)); gold.extend(yb.numpy())\n",
        "probs = np.vstack(probs)\n",
        "\n",
        "print(classification_report(gold, preds, target_names=[\"Engaged\",\"Hesitating\",\"Rejecting\"]))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(gold, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl0tkasnf3FW",
        "outputId": "fe61379a-619d-425d-de30-ef30e478cf80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Engaged       0.64      1.00      0.78      1658\n",
            "  Hesitating       0.00      0.00      0.00       539\n",
            "   Rejecting       0.00      0.00      0.00       413\n",
            "\n",
            "    accuracy                           0.64      2610\n",
            "   macro avg       0.21      0.33      0.26      2610\n",
            "weighted avg       0.40      0.64      0.49      2610\n",
            "\n",
            "Confusion matrix:\n",
            " [[1658    0    0]\n",
            " [ 539    0    0]\n",
            " [ 413    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class balance\n",
        "import numpy as np\n",
        "dtr = np.load(\"/content/features/meld_train.npz\"); ytr = dtr[\"y\"]\n",
        "ddv = np.load(\"/content/features/meld_dev.npz\");   ydv = ddv[\"y\"]\n",
        "dte = np.load(\"/content/features/meld_test.npz\");  yte = dte[\"y\"]\n",
        "\n",
        "def counts(y):\n",
        "    u,c = np.unique(y, return_counts=True);\n",
        "    return dict(zip(u,c)), c.sum()\n",
        "\n",
        "print(\"Train:\", counts(ytr))  # {0:…,1:…,2:…}, total\n",
        "print(\"Dev:  \", counts(ydv))\n",
        "print(\"Test: \", counts(yte))\n"
      ],
      "metadata": {
        "id": "KsBfh1ZVjoIO",
        "outputId": "716b0536-9c83-4bf3-a07d-dd81d97941da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: ({np.int64(0): np.int64(6453), np.int64(1): np.int64(2156), np.int64(2): np.int64(1380)}, np.int64(9989))\n",
            "Dev:   ({np.int64(0): np.int64(633), np.int64(1): np.int64(301), np.int64(2): np.int64(175)}, np.int64(1109))\n",
            "Test:  ({np.int64(0): np.int64(1658), np.int64(1): np.int64(539), np.int64(2): np.int64(413)}, np.int64(2610))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Xtr,ytr = np.load(\"/content/features/meld_train.npz\")[\"X\"], np.load(\"/content/features/meld_train.npz\")[\"y\"]\n",
        "Xdv,ydv = np.load(\"/content/features/meld_dev.npz\")[\"X\"],   np.load(\"/content/features/meld_dev.npz\")[\"y\"]\n",
        "\n",
        "tr = TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr))\n",
        "dv = TensorDataset(torch.from_numpy(Xdv), torch.from_numpy(ydv))\n",
        "\n",
        "# class weights (inverse freq)\n",
        "counts = np.bincount(ytr, minlength=3).astype(np.float32)\n",
        "class_w = (counts.sum() / (3.0 * np.maximum(counts, 1)))  # balances classes\n",
        "cw = torch.tensor(class_w, dtype=torch.float32, device=device)\n",
        "\n",
        "# balanced sampling\n",
        "sample_w = class_w[ytr]\n",
        "sampler = WeightedRandomSampler(weights=torch.from_numpy(sample_w), num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(tr, batch_size=64, sampler=sampler)\n",
        "dev_loader   = DataLoader(dv, batch_size=128, shuffle=False)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, d_in=1024, d_h=256, d_h2=64, n_out=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_in,d_h), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(d_h,d_h2), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(d_h2,n_out)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "model = Head().to(device)\n",
        "crit  = nn.CrossEntropyLoss(weight=cw)  # <= weighted loss\n",
        "opt   = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "best_f1, wait, patience = 0, 0, 6\n",
        "for ep in range(60):\n",
        "    model.train()\n",
        "    for xb,yb in train_loader:\n",
        "        xb,yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad(); loss = crit(model(xb), yb); loss.backward(); opt.step()\n",
        "    # dev macro-F1\n",
        "    model.eval(); preds=[]; gold=[]\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in dev_loader:\n",
        "            pb = model(xb.to(device)).softmax(1).argmax(1).cpu().numpy()\n",
        "            preds.extend(pb); gold.extend(yb.numpy())\n",
        "    f1 = f1_score(gold, preds, average=\"macro\")\n",
        "    print(f\"Epoch {ep:02d}  dev Macro-F1 = {f1:.3f}\")\n",
        "    if f1 > best_f1: best_f1, wait = f1, 0; torch.save(model.state_dict(), \"/content/features/head_best.pt\")\n",
        "    else: wait += 1\n",
        "    if wait >= patience: print(\"Early stop.\"); break\n",
        "\n",
        "print(\"Best dev Macro-F1:\", round(best_f1,3))\n"
      ],
      "metadata": {
        "id": "UtxT18TYsF3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}