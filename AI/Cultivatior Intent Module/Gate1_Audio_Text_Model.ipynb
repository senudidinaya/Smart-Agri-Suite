{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/senudidinaya/Smart-Agri-Suite/blob/main/AI/Cultivatior%20Intent%20Module/Gate1_Audio_Text_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrPUhcJLAp_1"
      },
      "source": [
        "## **Gate-1 Voice + Text Intent Classification Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FmVNKbt7Afno"
      },
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "# Install compatible versions\n",
        "!pip -q install --upgrade --no-cache-dir \"transformers==4.39.3\" \"huggingface_hub>=0.23\" accelerate\n",
        "\n",
        "# Imports (after pip cell finishes running)\n",
        "import torch, numpy as np\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2AamafoZRdJU"
      },
      "outputs": [],
      "source": [
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBEP7hRvScTC",
        "outputId": "e997f735-12f3-4887-9c8d-48322e17221f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Audio: Use Wav2Vec2 Feature Extractor instead of Processor\n",
        "fe = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "xlsr = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\").to(device).eval()\n",
        "\n",
        "# Text: BERT base\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(device).eval()\n",
        "\n",
        "# ASR: Whisper base\n",
        "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=0 if torch.cuda.is_available() else -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QZIUuXUVbrPU"
      },
      "outputs": [],
      "source": [
        "# Audio + Text Featurizer\n",
        "import os, hashlib\n",
        "import torch.nn.functional as F\n",
        "import soundfile as sf\n",
        "import torchaudio\n",
        "\n",
        "TEXT_CACHE = \"/content/features/text_embeds_cache\"\n",
        "os.makedirs(TEXT_CACHE, exist_ok=True)\n",
        "\n",
        "# Util to generate a hash for each audio file (used as cache key)\n",
        "def audio_hash(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "# Use Whisper to get ASR transcript\n",
        "def transcribe_whisper(path):\n",
        "    return asr(path, language=\"en\")[\"text\"]\n",
        "\n",
        "# Get text embedding using BERT\n",
        "@torch.no_grad()\n",
        "def bert_embed(text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
        "    h = bert(**tokens).last_hidden_state\n",
        "    return F.adaptive_avg_pool1d(h.transpose(1,2), 1).squeeze().cpu().numpy()  # (768,)\n",
        "\n",
        "@torch.no_grad()\n",
        "def xlsr_embed(audio, sr=1600):\n",
        "    inputs = fe(audio, sampling_rate=sr, return_tensors=\"pt\").input_values.to(device)\n",
        "    h = xlsr(inputs).last_hidden_state\n",
        "    return F.adaptive_avg_pool1d(h.transpose(1,2), 1).squeeze().cpu().numpy()\n",
        "\n",
        "def load_wav(path, sr=16000):\n",
        "    y, s = sf.read(path)\n",
        "    if s != sr:\n",
        "        y = torchaudio.functional.resample(torch.tensor(y).float().unsqueeze(0), s, sr).squeeze(0).numpy()\n",
        "    if y.ndim > 1: y = y.mean(-1)\n",
        "    return y\n",
        "\n",
        "# Complete featurizer: XLSR audio + BERT text\n",
        "def featurize(path, sr=16000):\n",
        "    audio = load_wav(path, sr).astype(np.float32)\n",
        "    a_feat = xlsr_embed(audio, sr)  # (1024,)\n",
        "\n",
        "    # Cache logic\n",
        "    key = os.path.join(TEXT_CACHE, f\"{audio_hash(path)}.npy\")\n",
        "    if os.path.exists(key):\n",
        "        t_feat = np.load(key)\n",
        "    else:\n",
        "        text = transcribe_whisper(path)\n",
        "        t_feat = bert_embed(text)\n",
        "        np.save(key, t_feat)\n",
        "\n",
        "    return np.concatenate([a_feat, t_feat])  # (1024 + 768 = 1792,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6q02sp8jf_P",
        "outputId": "2ac4bb1e-cac1-4907-e8af-390154d75c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/zaber666/meld-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading meld-dataset.zip to /content\n",
            "100% 11.0G/11.0G [02:33<00:00, 185MB/s]\n",
            "100% 11.0G/11.0G [02:33<00:00, 77.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download & Extract MELD Dataset\n",
        "KAGGLE_USERNAME = \"senudirupasinghe\"\n",
        "KAGGLE_KEY = \"7780e1bc02634783fb08137fa45db94e\"\n",
        "\n",
        "!pip install kaggle --upgrade --quiet\n",
        "!mkdir -p /root/.kaggle\n",
        "!echo '{\"username\":\"<KAGGLE_USERNAME>\",\"key\":\"<KAGGLE_KEY>\"}' > /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d zaber666/meld-dataset -p /content/ --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jC2BZh1Ml5jS"
      },
      "outputs": [],
      "source": [
        "# Extract CSV paths and audio mapping\n",
        "import os, glob, pandas as pd\n",
        "MELD_ROOT = \"/content/MELD.Raw\"\n",
        "AUDIO_OUT = \"/content/audio/meld\"\n",
        "\n",
        "# Index mp4 files\n",
        "mp4_index = {}\n",
        "for p in glob.glob(os.path.join(MELD_ROOT, \"**\", \"*.mp4\"), recursive=True):\n",
        "    mp4_index[os.path.basename(p).lower()] = p\n",
        "\n",
        "# Extract audio from MP4 to WAV\n",
        "import subprocess, soundfile as sf\n",
        "os.makedirs(AUDIO_OUT, exist_ok=True)\n",
        "def mp4_to_wav(dialogue_id, utterance_id):\n",
        "    mp4 = mp4_index.get(f\"dia{dialogue_id}_utt{utterance_id}.mp4\")\n",
        "    if not mp4: return None\n",
        "    out_path = os.path.join(AUDIO_OUT, f\"{dialogue_id}_{utterance_id}.wav\")\n",
        "    if not os.path.exists(out_path):\n",
        "        subprocess.run([\"ffmpeg\",\"-y\",\"-i\",mp4,\"-vn\",\"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\",out_path],\n",
        "                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    return out_path if os.path.exists(out_path) else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sWmSMl92Tt0A"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import subprocess\n",
        "\n",
        "AUDIO_OUT = \"/content/audio/meld\"\n",
        "os.makedirs(AUDIO_OUT, exist_ok=True)\n",
        "\n",
        "# Build a fast lookup once\n",
        "mp4_index = {}\n",
        "for p in glob.glob(\"/content/MELD-RAW/MELD.Raw/**/*.mp4\", recursive=True):\n",
        "    mp4_index[os.path.basename(p).lower()] = p\n",
        "\n",
        "def mp4_to_wav(dialogue_id, utterance_id):\n",
        "    name = f\"dia{dialogue_id}_utt{utterance_id}.mp4\".lower()\n",
        "    src = mp4_index.get(name)\n",
        "    if not src:\n",
        "        return None\n",
        "    dst = os.path.join(AUDIO_OUT, f\"{dialogue_id}_{utterance_id}.wav\")\n",
        "    if not os.path.exists(dst):\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\", \"-y\", \"-i\", src, \"-vn\",\n",
        "            \"-ac\", \"1\", \"-ar\", \"16000\", \"-sample_fmt\", \"s16\", dst\n",
        "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    return dst if os.path.exists(dst) else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fbmMDlKSl-KE"
      },
      "outputs": [],
      "source": [
        "# Generate Manifest CSVs\n",
        "def build_manifest(csv_path, out_path, lang=\"en\", domain=\"general\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    paths, labels = [], []\n",
        "    emap = {\n",
        "        \"engaged\":    {\"neutral\",\"joy\",\"happy\",\"excited\"},\n",
        "        \"hesitating\": {\"sadness\",\"fear\",\"surprise\"},\n",
        "        \"rejecting\":  {\"anger\",\"disgust\",\"contempt\"}\n",
        "    }\n",
        "    label_map = {\"engaged\":0,\"hesitating\":1,\"rejecting\":2}\n",
        "    for i, row in df.iterrows():\n",
        "        emo = row[\"Emotion\"].lower().strip()\n",
        "        label = None\n",
        "        for k,v in emap.items():\n",
        "            if emo in v: label = label_map[k]\n",
        "        if label is None: continue\n",
        "        wav = mp4_to_wav(row[\"Dialogue_ID\"], row[\"Utterance_ID\"])\n",
        "        if wav: paths.append(wav); labels.append(label)\n",
        "    pd.DataFrame({\n",
        "        \"path\": paths,\n",
        "        \"label\": labels,\n",
        "        \"lang\": lang,\n",
        "        \"domain\": domain\n",
        "    }).to_csv(out_path, index=False)\n",
        "\n",
        "build_manifest(\"/content/MELD-RAW/MELD.Raw/train/train_sent_emo.csv\", \"/content/manifest_meld_train.csv\")\n",
        "build_manifest(\"/content/MELD-RAW/MELD.Raw/dev_sent_emo.csv\", \"/content/manifest_meld_dev.csv\")\n",
        "build_manifest(\"/content/MELD-RAW/MELD.Raw/test_sent_emo.csv\", \"/content/manifest_meld_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0_Rf63HdAiu",
        "outputId": "10820b48-f18a-44c9-b2ce-3e0796219c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Featurize manifest_meld_train.csv: 100%|██████████| 9988/9988 [09:57<00:00, 16.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/features/meld_train.npz (9988, 1792) (9988,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Featurize manifest_meld_dev.csv: 100%|██████████| 1109/1109 [01:04<00:00, 17.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/features/meld_dev.npz (1109, 1792) (1109,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Featurize manifest_meld_test.csv: 100%|██████████| 2609/2609 [02:31<00:00, 17.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/features/meld_test.npz (2609, 1792) (2609,)\n"
          ]
        }
      ],
      "source": [
        "# Rebuild combined feature arrays (audio + text)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def build_npz(manifest_csv, out_npz):\n",
        "    man = pd.read_csv(manifest_csv)\n",
        "    X, y = [], []\n",
        "    for p, lab in tqdm(zip(man[\"path\"], man[\"label\"]), total=len(man), desc=f\"Featurize {os.path.basename(manifest_csv)}\"):\n",
        "        try:\n",
        "            x = featurize(p)\n",
        "            X.append(x); y.append(int(lab))\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Failed: {p} -> {e}\")\n",
        "    X = np.stack(X).astype(np.float32)\n",
        "    y = np.array(y, dtype=np.int64)\n",
        "    np.savez(out_npz, X=X, y=y)\n",
        "    print(out_npz, X.shape, y.shape)\n",
        "\n",
        "# Trigger rebuilds (may take 10+ mins depending on GPU/CPU)\n",
        "build_npz(\"/content/manifest_meld_train.csv\", \"/content/features/meld_train.npz\")\n",
        "build_npz(\"/content/manifest_meld_dev.csv\", \"/content/features/meld_dev.npz\")\n",
        "build_npz(\"/content/manifest_meld_test.csv\", \"/content/features/meld_test.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_model(train_npz, dev_npz, save_path):\n",
        "    Xtr, ytr = np.load(train_npz)[\"X\"], np.load(train_npz)[\"y\"]\n",
        "    Xdv, ydv = np.load(dev_npz)[\"X\"], np.load(dev_npz)[\"y\"]\n",
        "\n",
        "    tr = TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr))\n",
        "    dv = TensorDataset(torch.from_numpy(Xdv), torch.from_numpy(ydv))\n",
        "\n",
        "    # Balanced sampling\n",
        "    counts = np.bincount(ytr, minlength=3).astype(np.float32)\n",
        "    class_w = (counts.sum() / (3.0 * np.maximum(counts, 1)))\n",
        "    cw = torch.tensor(class_w, dtype=torch.float32, device=device)\n",
        "\n",
        "    sample_w = class_w[ytr]\n",
        "    sampler = WeightedRandomSampler(weights=torch.from_numpy(sample_w), num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(tr, batch_size=64, sampler=sampler)\n",
        "    dev_loader = DataLoader(dv, batch_size=128, shuffle=False)\n",
        "\n",
        "    class Head(nn.Module):\n",
        "        def __init__(self, d_in=1792, d_h=256, d_h2=64, n_out=3):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(d_in,d_h), nn.ReLU(), nn.Dropout(0.2),\n",
        "                nn.Linear(d_h,d_h2), nn.ReLU(), nn.Dropout(0.2),\n",
        "                nn.Linear(d_h2,n_out)\n",
        "            )\n",
        "        def forward(self, x): return self.net(x)\n",
        "\n",
        "    model = Head().to(device)\n",
        "    crit = nn.CrossEntropyLoss(weight=cw)\n",
        "    opt = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "    best_f1, wait, patience = 0, 0, 6\n",
        "    for ep in range(60):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(); loss = crit(model(xb), yb); loss.backward(); opt.step()\n",
        "        model.eval(); preds, gold = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dev_loader:\n",
        "                pb = model(xb.to(device)).softmax(1).argmax(1).cpu().numpy()\n",
        "                preds.extend(pb); gold.extend(yb.numpy())\n",
        "        f1 = f1_score(gold, preds, average=\"macro\")\n",
        "        print(f\"Epoch {ep:02d}  dev Macro-F1 = {f1:.3f}\")\n",
        "        if f1 > best_f1: best_f1, wait = f1, 0; torch.save(model.state_dict(), save_path)\n",
        "        else: wait += 1\n",
        "        if wait >= patience: print(\"Early stop.\"); break\n",
        "\n",
        "    print(\"Best dev Macro-F1:\", round(best_f1, 3))"
      ],
      "metadata": {
        "id": "zMyE89VGloqd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    \"/content/features/meld_train.npz\",\n",
        "    \"/content/features/meld_dev.npz\",\n",
        "    \"/content/features/head_best.pt\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjtCTuQOedQ8",
        "outputId": "3603b1ef-a2e6-4cd2-b837-67dedff31aed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00  dev Macro-F1 = 0.164\n",
            "Epoch 01  dev Macro-F1 = 0.207\n",
            "Epoch 02  dev Macro-F1 = 0.229\n",
            "Epoch 03  dev Macro-F1 = 0.229\n",
            "Epoch 04  dev Macro-F1 = 0.230\n",
            "Epoch 05  dev Macro-F1 = 0.228\n",
            "Epoch 06  dev Macro-F1 = 0.234\n",
            "Epoch 07  dev Macro-F1 = 0.231\n",
            "Epoch 08  dev Macro-F1 = 0.229\n",
            "Epoch 09  dev Macro-F1 = 0.245\n",
            "Epoch 10  dev Macro-F1 = 0.264\n",
            "Epoch 11  dev Macro-F1 = 0.235\n",
            "Epoch 12  dev Macro-F1 = 0.284\n",
            "Epoch 13  dev Macro-F1 = 0.255\n",
            "Epoch 14  dev Macro-F1 = 0.281\n",
            "Epoch 15  dev Macro-F1 = 0.295\n",
            "Epoch 16  dev Macro-F1 = 0.302\n",
            "Epoch 17  dev Macro-F1 = 0.275\n",
            "Epoch 18  dev Macro-F1 = 0.268\n",
            "Epoch 19  dev Macro-F1 = 0.268\n",
            "Epoch 20  dev Macro-F1 = 0.296\n",
            "Epoch 21  dev Macro-F1 = 0.302\n",
            "Epoch 22  dev Macro-F1 = 0.311\n",
            "Epoch 23  dev Macro-F1 = 0.313\n",
            "Epoch 24  dev Macro-F1 = 0.303\n",
            "Epoch 25  dev Macro-F1 = 0.298\n",
            "Epoch 26  dev Macro-F1 = 0.307\n",
            "Epoch 27  dev Macro-F1 = 0.292\n",
            "Epoch 28  dev Macro-F1 = 0.313\n",
            "Epoch 29  dev Macro-F1 = 0.318\n",
            "Epoch 30  dev Macro-F1 = 0.318\n",
            "Epoch 31  dev Macro-F1 = 0.320\n",
            "Epoch 32  dev Macro-F1 = 0.316\n",
            "Epoch 33  dev Macro-F1 = 0.340\n",
            "Epoch 34  dev Macro-F1 = 0.328\n",
            "Epoch 35  dev Macro-F1 = 0.330\n",
            "Epoch 36  dev Macro-F1 = 0.334\n",
            "Epoch 37  dev Macro-F1 = 0.305\n",
            "Epoch 38  dev Macro-F1 = 0.350\n",
            "Epoch 39  dev Macro-F1 = 0.344\n",
            "Epoch 40  dev Macro-F1 = 0.343\n",
            "Epoch 41  dev Macro-F1 = 0.344\n",
            "Epoch 42  dev Macro-F1 = 0.346\n",
            "Epoch 43  dev Macro-F1 = 0.362\n",
            "Epoch 44  dev Macro-F1 = 0.339\n",
            "Epoch 45  dev Macro-F1 = 0.359\n",
            "Epoch 46  dev Macro-F1 = 0.344\n",
            "Epoch 47  dev Macro-F1 = 0.340\n",
            "Epoch 48  dev Macro-F1 = 0.364\n",
            "Epoch 49  dev Macro-F1 = 0.350\n",
            "Epoch 50  dev Macro-F1 = 0.343\n",
            "Epoch 51  dev Macro-F1 = 0.367\n",
            "Epoch 52  dev Macro-F1 = 0.354\n",
            "Epoch 53  dev Macro-F1 = 0.381\n",
            "Epoch 54  dev Macro-F1 = 0.358\n",
            "Epoch 55  dev Macro-F1 = 0.364\n",
            "Epoch 56  dev Macro-F1 = 0.380\n",
            "Epoch 57  dev Macro-F1 = 0.390\n",
            "Epoch 58  dev Macro-F1 = 0.387\n",
            "Epoch 59  dev Macro-F1 = 0.372\n",
            "Best dev Macro-F1: 0.39\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1OF_2eGSNli2J2_b3fjwvos_kR2JBNnKV",
      "authorship_tag": "ABX9TyOI7R1zQLeL+fAMEu3evu/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}